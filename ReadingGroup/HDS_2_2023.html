<!DOCTYPE html>
<html>
<head>
<title>High-Dimensional Statistics</title>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</head>
<body>

<h1>Reading Group: High-Dimensional Statistics.</h1>

<p>Hola. Durante el segundo semestre de 2023 estaremos leyendo el libro High-Dimensional Statistics de de Martin J. Wainwright.<p>
<br>
<p> Nos reunimos (usualmente) los Martes a las 14:30 en la Sala IDEUV del Instituto de Estadística de la Facultad de Ciencias de la UV, excepto cuando hay consejo de profesores o alguna actividad extra, donde nos reunimos a las 13:30 o simplemente no nos reunimos.</p>

<br>
<p> Ante cualquier duda no duden en escribir a nicolas dot rivera at uv dot cl.


<h2>Sesiones </h2>

<ul>
  <li><strong>14 Noviembre 2023:</strong> Estudiamos en detalle el ejemplo 8.3 ya que hubo una discusión muy interesante la vez anterior. Estudiamos la solución usando PCA, pero también escribí un código para encontrar los parámetros con EM  
  <ul> 
    <li> Codigo en python de algoritmo EM y solución PCA para ejemplo 8.3: <a href = "PCA_examples.py"> Código</a></il>
  </ul>
  </li> 
<br>
<li><strong>7 Noviembre 2023:</strong> Empezamos el capítulo 8 de Principal Component Analysis (capítulo 8). Partimos hablando de valores y vectores propios, y el teorema de Courant-Fisher (que no aparece en el libro, parece). Vimos ejemplos e interpretaciones de PCA (Sección 8.1.1). También hablamos de perturbaciones de valores y vectores propios (Sección 8.1.2).  </li> 
 <br>
<li><strong>17 Octubre 2023:</strong> Estudiamos condiciones necesarias para satisfacer la restricted nullspace property (Sección 7.2.3). En particular la Restricted isometry property (Definición 7.10 y Proposición 7.11)</li> 
 <br>
<li><strong>10 Octubre 2023:</strong> Vimos cuando el BPLP (La relajación \(L_1\)) resuelve el problema \(L_0\), que ocurre si y solo si se satisface la restricted nullspace property (Teorema 7.8).
 <br>
 <br> 
<li><strong>26 Septiembre 2023:</strong> Vimos un poco más de intuición sobre las regularizaciones \(L_1\) y \(L_2\). Volvimos a discutir la sección 7.2 con más énfasis en los problemas de optimización, y la relajación del problema de minimización en \(L_0\) al problema en \(L_1\) (Seción 7.2.1), y discutimos un poco más el Basis Pursuit Linear Program</li> 
<ul> <li> Código en R de un modelo "complejo" pero que en realidad tiene baja dimensionalidad. Vimos el efecto de incorporar penalizaciones en \(L_1\) y \(L_2\): <a href = "Example_session3.R"> Código</a></il>
</ul>
  <br>
<li><strong>12 Septiembre 2023:</strong> Empezamos a ver el Capítulo 7 del libro, en particular la Sección 7.2 </li>  
  <br>
  <li><strong>22 Agosto 2023:</strong> Capítulo 1 del libro. Vimos los ejemplos de las secciones 1.2.2 y 1.2.3, y 1.3.2
  <ul> <li> Codigo Ejemplo 1.2.2 y solución usando threshold estimators (1.3.2): <a href = "Example_covariance.R"> Código</a></il>
  <li> Codigo Ejemplo 1.2.3: <a href = "Example_Distance.R"> Código</a></il>
  </ul>
  </li>
</ul>


</body>
</html>
